{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:12:59.616797Z",
     "start_time": "2019-11-27T12:12:58.270238Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from src.train_and_evaluate import *\n",
    "from src.models import *\n",
    "import time\n",
    "import torch.optim\n",
    "from src.expressions_transfer import *\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path,'r') as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "def write_json(path,file):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:13:18.841736Z",
     "start_time": "2019-11-27T12:13:18.829288Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_fold(ori_path,prefix,data,pairs,group):\n",
    "    mode_train = 'train'\n",
    "    mode_valid = 'valid'\n",
    "    mode_test = 'test'\n",
    "    train_path = ori_path + mode_train + prefix\n",
    "    valid_path = ori_path + mode_valid + prefix\n",
    "    test_path = ori_path + mode_test + prefix\n",
    "    train = read_json(train_path)\n",
    "    train_id = [item['id'] for item in train]\n",
    "    valid = read_json(valid_path)\n",
    "    valid_id = [item['id'] for item in valid]\n",
    "    test = read_json(test_path)\n",
    "    test_id = [item['id'] for item in test]\n",
    "    train_fold = []\n",
    "    valid_fold = []\n",
    "    test_fold = []\n",
    "    for item,pair,g in zip(data, pairs, group):\n",
    "        pair = list(pair)\n",
    "        pair.append(g['group_num'])\n",
    "        pair = tuple(pair)\n",
    "        if item['id'] in train_id:\n",
    "            train_fold.append(pair)\n",
    "        elif item['id'] in test_id:\n",
    "            test_fold.append(pair)\n",
    "        else:\n",
    "            valid_fold.append(pair)\n",
    "    return train_fold, test_fold, valid_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:13:19.789441Z",
     "start_time": "2019-11-27T12:13:19.780071Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_raw_data(filename):  # load the json data to list(dict()) for MATH 23K\n",
    "    print(\"Reading lines...\")\n",
    "    f = open(filename, encoding=\"utf-8\")\n",
    "    js = \"\"\n",
    "    data = []\n",
    "    for i, s in enumerate(f):\n",
    "        js += s\n",
    "        i += 1\n",
    "        if i % 7 == 0:  # every 7 line is a json\n",
    "            data_d = json.loads(js)\n",
    "            if \"千米/小时\" in data_d[\"equation\"]:\n",
    "                data_d[\"equation\"] = data_d[\"equation\"][:-5]\n",
    "            data.append(data_d)\n",
    "            js = \"\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:13:42.590706Z",
     "start_time": "2019-11-27T12:13:42.567440Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums,group_train, group_test, tree=False):\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs_trained:\n",
    "        if not tree:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "        elif pair[-1]:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "    input_lang.build_input_lang(trim_min_count)\n",
    "    if tree:\n",
    "        output_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n",
    "    else:\n",
    "        output_lang.build_output_lang(generate_nums, copy_nums)\n",
    "\n",
    "    for pair in pairs_trained:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                            pair[2], pair[3], num_stack))\n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    print('Number of training data %d' % (len(train_pairs)))\n",
    "    for pair in pairs_tested:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        test_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                           pair[2], pair[3], num_stack))\n",
    "    print('Number of testind data %d' % (len(test_pairs)))\n",
    "    return input_lang, output_lang, train_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:45:34.461173Z",
     "start_time": "2019-11-27T12:45:34.449355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5, 0.02, 2.0]\n"
     ]
    }
   ],
   "source": [
    "def change_num(num):\n",
    "    new_num = []\n",
    "    for item in num:\n",
    "        if '/' in item:\n",
    "            new_str = item.split(')')[0]\n",
    "            new_str = new_str.split('(')[1]\n",
    "            a = float(new_str.split('/')[0])\n",
    "            b = float(new_str.split('/')[1])\n",
    "            value = a/b\n",
    "            new_num.append(value)\n",
    "        elif '%' in item:\n",
    "            value = float(item[0:-1])/100\n",
    "            new_num.append(value)\n",
    "        else:\n",
    "            new_num.append(float(item))\n",
    "    return new_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:04.432750Z",
     "start_time": "2019-11-27T12:45:54.097513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n",
      "Transfer numbers...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 128\n",
    "hidden_size = 512\n",
    "n_epochs = 80\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "beam_size = 5\n",
    "n_layers = 2\n",
    "ori_path = '../graph_quantity_multigraph_trans/data/'\n",
    "prefix = '23k_processed.json'\n",
    "\n",
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")\n",
    "group_data = read_json(\"data/Math_23K_processed.json\")\n",
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")\n",
    "\n",
    "pairs, generate_nums, copy_nums = transfer_num(data)\n",
    "\n",
    "temp_pairs = []\n",
    "for p in pairs:\n",
    "    temp_pairs.append((p[0], from_infix_to_prefix(p[1]), change_num(p[2]), p[3]))\n",
    "pairs = temp_pairs\n",
    "\n",
    "train_fold, test_fold, valid_fold = get_train_test_fold(ori_path,prefix,data,pairs,group_data)\n",
    "\n",
    "\n",
    "best_acc_fold = []\n",
    "\n",
    "pairs_tested = test_fold\n",
    "pairs_trained = train_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:49.864021Z",
     "start_time": "2019-11-27T12:46:39.306073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing words...\n",
      "keep_words 3928 / 10543 = 0.3726\n",
      "Indexed 3931 words in input language, 23 words in output\n",
      "Number of training data 21162\n",
      "Number of testind data 1000\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums, tree=False):\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs_trained:\n",
    "        if not tree:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "        elif pair[-1]:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "    input_lang.build_input_lang(trim_min_count)\n",
    "    if tree:\n",
    "        output_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n",
    "    else:\n",
    "        output_lang.build_output_lang(generate_nums, copy_nums)\n",
    "\n",
    "    for pair in pairs_trained:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                            pair[2], pair[3], num_stack, pair[4]))\n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    print('Number of training data %d' % (len(train_pairs)))\n",
    "    for pair in pairs_tested:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        test_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                           pair[2], pair[3], num_stack,pair[4]))\n",
    "    print('Number of testind data %d' % (len(test_pairs)))\n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepare_data(pairs_trained, pairs_tested, 5, generate_nums,\n",
    "                                                                copy_nums, tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:14:08.886494Z",
     "start_time": "2019-11-27T12:14:08.876602Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_train_batch(pairs_to_batch, batch_size):\n",
    "    pairs = copy.deepcopy(pairs_to_batch)\n",
    "    random.shuffle(pairs)  # shuffle the pairs\n",
    "    pos = 0\n",
    "    input_lengths = []\n",
    "    output_lengths = []\n",
    "    nums_batches = []\n",
    "    batches = []\n",
    "    input_batches = []\n",
    "    output_batches = []\n",
    "    num_stack_batches = []  # save the num stack which\n",
    "    num_pos_batches = []\n",
    "    num_size_batches = []\n",
    "    group_batches = []\n",
    "    num_value_batches = []\n",
    "    while pos + batch_size < len(pairs):\n",
    "        batches.append(pairs[pos:pos+batch_size])\n",
    "        pos += batch_size\n",
    "    batches.append(pairs[pos:])\n",
    "\n",
    "    for batch in batches:\n",
    "        batch = sorted(batch, key=lambda tp: tp[1], reverse=True)\n",
    "        input_length = []\n",
    "        output_length = []\n",
    "        for _, i, _, j, _, _, _,_ in batch:\n",
    "            input_length.append(i)\n",
    "            output_length.append(j)\n",
    "        input_lengths.append(input_length)\n",
    "        output_lengths.append(output_length)\n",
    "        input_len_max = input_length[0]\n",
    "        output_len_max = max(output_length)\n",
    "        input_batch = []\n",
    "        output_batch = []\n",
    "        num_batch = []\n",
    "        num_stack_batch = []\n",
    "        num_pos_batch = []\n",
    "        num_size_batch = []\n",
    "        group_batch = []\n",
    "        num_value_batch = []\n",
    "        for i, li, j, lj, num, num_pos, num_stack, group in batch:\n",
    "            num_batch.append(len(num))\n",
    "            input_batch.append(pad_seq(i, li, input_len_max))\n",
    "            output_batch.append(pad_seq(j, lj, output_len_max))\n",
    "            num_stack_batch.append(num_stack)\n",
    "            num_pos_batch.append(num_pos)\n",
    "            num_size_batch.append(len(num_pos))\n",
    "            num_value_batch.append(num)\n",
    "            group_batch.append(group)\n",
    "        input_batches.append(input_batch)\n",
    "        nums_batches.append(num_batch)\n",
    "        output_batches.append(output_batch)\n",
    "        num_stack_batches.append(num_stack_batch)\n",
    "        num_pos_batches.append(num_pos_batch)\n",
    "        num_size_batches.append(num_size_batch)\n",
    "        num_value_batches.append(num_value_batch)\n",
    "        group_batches.append(group_batch)\n",
    "        \n",
    "    return input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, group_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:51.135145Z",
     "start_time": "2019-11-27T12:46:49.866107Z"
    }
   },
   "outputs": [],
   "source": [
    "input_batches, input_lengths, output_batches, output_lengths, nums_batches, \\\n",
    "   num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, group_batches = prepare_train_batch(train_pairs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Test Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T10:59:11.915005Z",
     "start_time": "2019-11-27T10:59:11.911506Z"
    }
   },
   "source": [
    "## Build Model and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:53.316259Z",
     "start_time": "2019-11-27T12:46:53.113759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=embedding_size, hidden_size=hidden_size,\n",
    "                     n_layers=n_layers)\n",
    "predict = Prediction(hidden_size=hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums),\n",
    "                     input_size=len(generate_nums))\n",
    "generate = GenerateNode(hidden_size=hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums),\n",
    "                        embedding_size=embedding_size)\n",
    "merge = Merge(hidden_size=hidden_size, embedding_size=embedding_size)\n",
    "# the embedding layer is  only for generated number embeddings, operators, and paddings\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "predict_optimizer = torch.optim.Adam(predict.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "generate_optimizer = torch.optim.Adam(generate.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "merge_optimizer = torch.optim.Adam(merge.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n",
    "predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n",
    "generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n",
    "merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    predict.cuda()\n",
    "    generate.cuda()\n",
    "    merge.cuda()\n",
    "\n",
    "generate_num_ids = []\n",
    "for num in generate_nums:\n",
    "    generate_num_ids.append(output_lang.word2index[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:14:19.404905Z",
     "start_time": "2019-11-27T12:14:19.401911Z"
    }
   },
   "outputs": [],
   "source": [
    "#for epoch in range(n_epochs):\n",
    "encoder_scheduler.step()\n",
    "predict_scheduler.step()\n",
    "generate_scheduler.step()\n",
    "merge_scheduler.step()\n",
    "loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:54.417978Z",
     "start_time": "2019-11-27T12:46:54.408982Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,\n",
    "               encoder, predict, generate, merge, encoder_optimizer, predict_optimizer, generate_optimizer,\n",
    "               merge_optimizer, output_lang, num_pos, num_value, group) = (input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n",
    "    num_stack_batches[idx], num_size_batches[idx], generate_num_ids, encoder, predict, generate, merge,\n",
    "    encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx], num_value_batches[idx], group_batches[idx])\n",
    "english=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:58.125460Z",
     "start_time": "2019-11-27T12:46:58.120420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 50, 47, 46, 45, 44, 42, 42, 41, 39, 36, 33, 32, 32, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 27, 27, 27, 26, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 21, 20, 20, 18, 17, 17, 17, 17, 15, 15, 11, 11, 10, 10, 9]\n"
     ]
    }
   ],
   "source": [
    "print(input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_tree Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:00.262425Z",
     "start_time": "2019-11-27T12:47:00.255401Z"
    }
   },
   "outputs": [],
   "source": [
    "# sequence mask for attention\n",
    "seq_mask = []\n",
    "max_len = max(input_length)\n",
    "for i in input_length:\n",
    "    seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n",
    "seq_mask = torch.ByteTensor(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:02.748740Z",
     "start_time": "2019-11-27T12:47:02.738472Z"
    }
   },
   "outputs": [],
   "source": [
    "num_mask = []\n",
    "max_num_size = max(num_size_batch) + len(generate_nums)\n",
    "for i in num_size_batch:\n",
    "    d = i + len(generate_nums)\n",
    "    num_mask.append([0] * d + [1] * (max_num_size - d))\n",
    "num_mask = torch.ByteTensor(num_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:03.760241Z",
     "start_time": "2019-11-27T12:47:03.752171Z"
    }
   },
   "outputs": [],
   "source": [
    "unk = output_lang.word2index[\"UNK\"]\n",
    "\n",
    "# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "input_var = torch.LongTensor(input_batch).transpose(0, 1)\n",
    "\n",
    "target = torch.LongTensor(target_batch).transpose(0, 1)\n",
    "\n",
    "padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n",
    "batch_size = len(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:04.998165Z",
     "start_time": "2019-11-27T12:47:04.975830Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "predict.train()\n",
    "generate.train()\n",
    "merge.train()\n",
    "\n",
    "if USE_CUDA:\n",
    "    input_var = input_var.cuda()\n",
    "    seq_mask = seq_mask.cuda()\n",
    "    padding_hidden = padding_hidden.cuda()\n",
    "    num_mask = num_mask.cuda()\n",
    "\n",
    "# Zero gradients of both optimizers\n",
    "encoder_optimizer.zero_grad()\n",
    "predict_optimizer.zero_grad()\n",
    "generate_optimizer.zero_grad()\n",
    "merge_optimizer.zero_grad()\n",
    "# Run words through encoder\n",
    "\n",
    "encoder_outputs, problem_output = encoder(input_var, input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:08:45.496833Z",
     "start_time": "2019-11-27T13:08:45.476718Z"
    }
   },
   "outputs": [],
   "source": [
    "# num net graph\n",
    "def get_lower_num_graph(max_len, sentence_length, num_list, id_num_list,contain_zh_flag=True):\n",
    "    diag_ele = np.zeros(max_len)\n",
    "    for i in range(sentence_length):\n",
    "        diag_ele[i] = 1\n",
    "    graph = np.diag(diag_ele)\n",
    "    if not contain_zh_flag:\n",
    "        return graph\n",
    "    for i in range(len(id_num_list)):\n",
    "        for j in range(len(id_num_list)):\n",
    "            if float(num_list[i]) <= float(num_list[j]):\n",
    "                graph[id_num_list[i]][id_num_list[j]] = 1\n",
    "            else:\n",
    "                graph[id_num_list[j]][id_num_list[i]] = 1\n",
    "    return graph\n",
    "\n",
    "def get_greater_num_graph(max_len, sentence_length, num_list, id_num_list,contain_zh_flag=True):\n",
    "    diag_ele = np.zeros(max_len)\n",
    "    for i in range(sentence_length):\n",
    "        diag_ele[i] = 1\n",
    "    graph = np.diag(diag_ele)\n",
    "    if not contain_zh_flag:\n",
    "        return graph\n",
    "    for i in range(len(id_num_list)):\n",
    "        for j in range(len(id_num_list)):\n",
    "            if float(num_list[i]) > float(num_list[j]):\n",
    "                graph[id_num_list[i]][id_num_list[j]] = 1\n",
    "            else:\n",
    "                graph[id_num_list[j]][id_num_list[i]] = 1\n",
    "    return graph\n",
    "\n",
    "# quantity cell graph\n",
    "def get_quantity_graph(max_len, sentence_length, quantity_cell_list,contain_zh_flag=True):\n",
    "    diag_ele = np.zeros(max_len)\n",
    "    for i in range(sentence_length):\n",
    "        diag_ele[i] = 1\n",
    "    graph = np.diag(diag_ele)\n",
    "    if not contain_zh_flag:\n",
    "        return graph\n",
    "    for i in quantity_cell_list:\n",
    "        for j in quantity_cell_list:\n",
    "            graph[i][j] = 1\n",
    "            graph[j][i] = 1\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:29.230668Z",
     "start_time": "2019-11-27T12:47:29.143894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_graph = []\n",
    "max_len = max(input_length)\n",
    "for i in range(len(input_length)):\n",
    "    sentence_length = input_length[i]\n",
    "    quantity_cell_list = group[i]\n",
    "    num_list = num_value[i]\n",
    "    id_num_list = num_pos[i]\n",
    "    graph_newc = get_quantity_graph(max_len, sentence_length, quantity_cell_list)\n",
    "    graph_greater = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n",
    "    graph_lower = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n",
    "    graph_total = [graph_newc.tolist(),graph_greater.tolist(),graph_lower.tolist()]\n",
    "    batch_graph.append(graph_total)\n",
    "batch_graph = np.array(batch_graph)\n",
    "batch_graph = torch.LongTensor(batch_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:50:01.732219Z",
     "start_time": "2019-11-27T12:50:01.673814Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gcn = Graph_Module(hidden_size, hidden_size, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:53:53.688463Z",
     "start_time": "2019-11-27T12:53:53.683082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:54:10.599260Z",
     "start_time": "2019-11-27T12:54:10.594242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 51, 51])\n"
     ]
    }
   ],
   "source": [
    "print(batch_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:54:39.328277Z",
     "start_time": "2019-11-27T12:54:39.322973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 51, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.transpose(0, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:04:11.244181Z",
     "start_time": "2019-11-27T13:04:11.200023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Graph Module\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff,d_out, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Graph_Module(nn.Module):\n",
    "    def __init__(self, indim, hiddim, outdim, dropout=0.3):\n",
    "        super(Graph_Module, self).__init__()\n",
    "        '''\n",
    "        ## Variables:\n",
    "        - indim: dimensionality of input node features\n",
    "        - hiddim: dimensionality of the joint hidden embedding\n",
    "        - outdim: dimensionality of the output node features\n",
    "        - combined_feature_dim: dimensionality of the joint hidden embedding for graph\n",
    "        - K: number of graph nodes/objects on the image\n",
    "        '''\n",
    "        self.in_dim = indim\n",
    "        #self.combined_dim = outdim\n",
    "        \n",
    "        #self.edge_layer_1 = nn.Linear(indim, outdim)\n",
    "        #self.edge_layer_2 = nn.Linear(outdim, outdim)\n",
    "        \n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "        #self.edge_layer_1 = nn.utils.weight_norm(self.edge_layer_1)\n",
    "        #self.edge_layer_2 = nn.utils.weight_norm(self.edge_layer_2)\n",
    "        self.h = 4\n",
    "        self.d_k = outdim//self.h\n",
    "        \n",
    "        #layer = GCN(indim, hiddim, self.d_k, dropout)\n",
    "        self.graph = clones(GCN(indim, hiddim, self.d_k, dropout), 4)\n",
    "        \n",
    "        #self.Graph_0 = GCN(indim, hiddim, outdim//4, dropout)\n",
    "        #self.Graph_1 = GCN(indim, hiddim, outdim//4, dropout)\n",
    "        #self.Graph_2 = GCN(indim, hiddim, outdim//4, dropout)\n",
    "        #self.Graph_3 = GCN(indim, hiddim, outdim//4, dropout)\n",
    "        \n",
    "        self.feed_foward = PositionwiseFeedForward(indim, hiddim, outdim, dropout)\n",
    "        self.norm = LayerNorm(outdim)\n",
    "\n",
    "    def get_adj(self, graph_nodes):\n",
    "        '''\n",
    "        ## Inputs:\n",
    "        - graph_nodes (batch_size, K, in_feat_dim): input features\n",
    "        ## Returns:\n",
    "        - adjacency matrix (batch_size, K, K)\n",
    "        '''\n",
    "        self.K = graph_nodes.size(1)\n",
    "        graph_nodes = graph_nodes.contiguous().view(-1, self.in_dim)\n",
    "        \n",
    "        # layer 1\n",
    "        h = self.edge_layer_1(graph_nodes)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        # layer 2\n",
    "        h = self.edge_layer_2(h)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        # outer product\n",
    "        h = h.view(-1, self.K, self.combined_dim)\n",
    "        adjacency_matrix = torch.matmul(h, h.transpose(1, 2))\n",
    "        \n",
    "        adjacency_matrix = self.b_normal(adjacency_matrix)\n",
    "\n",
    "        return adjacency_matrix\n",
    "    \n",
    "    def normalize(self, A, symmetric=True):\n",
    "        '''\n",
    "        ## Inputs:\n",
    "        - adjacency matrix (K, K) : A\n",
    "        ## Returns:\n",
    "        - adjacency matrix (K, K) \n",
    "        '''\n",
    "        A = A + torch.eye(A.size(0)).cuda().float()\n",
    "        d = A.sum(1)\n",
    "        if symmetric:\n",
    "            # D = D^{-1/2}\n",
    "            D = torch.diag(torch.pow(d, -0.5))\n",
    "            return D.mm(A).mm(D)\n",
    "        else :\n",
    "            D = torch.diag(torch.pow(d,-1))\n",
    "            return D.mm(A)\n",
    "       \n",
    "    def b_normal(self, adj):\n",
    "        batch = adj.size(0)\n",
    "        for i in range(batch):\n",
    "            adj[i] = self.normalize(adj[i])\n",
    "        return adj\n",
    "\n",
    "    def forward(self, graph_nodes, graph):\n",
    "        '''\n",
    "        ## Inputs:\n",
    "        - graph_nodes (batch_size, K, in_feat_dim): input features\n",
    "        ## Returns:\n",
    "        - graph_encode_features (batch_size, K, out_feat_dim)\n",
    "        '''\n",
    "        nbatches = graph_nodes.size(0)\n",
    "        mbatches = graph.size(0)\n",
    "        if nbatches != mbatches:\n",
    "            graph_nodes = graph_nodes.transpose(0, 1)\n",
    "        # adj (batch_size, K, K): adjacency matrix\n",
    "        if not bool(graph.numel()):\n",
    "            adj = self.get_adj(graph_nodes)\n",
    "            adj_list = [adj,adj,adj,adj]\n",
    "        else:\n",
    "            adj = graph.float()\n",
    "            adj_list = [adj[:,0,:],adj[:,1,:],adj[:,2,:],adj[:,0,:]]\n",
    "        #print(adj)\n",
    "        \n",
    "        g_feature = \\\n",
    "            tuple([l(graph_nodes,x) for l, x in zip(self.graph,adj_list)])\n",
    "        \n",
    "        g_feature = self.norm(torch.cat(g_feature,2))\n",
    "        #print('g_feature')\n",
    "        #print(g_feature.shape)\n",
    "        \n",
    "        graph_encode_features = self.feed_foward(g_feature)\n",
    "        \n",
    "        return adj, graph_encode_features\n",
    "\n",
    "# GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feat_dim, nhid, out_feat_dim, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        '''\n",
    "        ## Inputs:\n",
    "        - graph_nodes (batch_size, K, in_feat_dim): input features\n",
    "        - adjacency matrix (batch_size, K, K)\n",
    "        ## Returns:\n",
    "        - gcn_enhance_feature (batch_size, K, out_feat_dim)\n",
    "        '''\n",
    "        self.gc1 = GraphConvolution(in_feat_dim, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, out_feat_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return x\n",
    "    \n",
    "# Graph_Conv\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        #print(input.shape)\n",
    "        #print(self.weight.shape)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        print('adj.shape')\n",
    "        print(adj.shape)\n",
    "        print('support.shape')\n",
    "        print(support.shape)\n",
    "        output = torch.matmul(adj, support)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:04:12.414641Z",
     "start_time": "2019-11-27T13:04:12.353736Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gcn = Graph_Module(hidden_size, hidden_size, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:05:16.189549Z",
     "start_time": "2019-11-27T13:05:16.153309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs = encoder_outputs.transpose(0, 1).cuda()\n",
    "batch_graph = batch_graph.cuda()\n",
    "_, encoder_outputs = gcn(encoder_outputs, batch_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:59:23.823874Z",
     "start_time": "2019-11-27T12:59:23.818559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 51, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:48:08.794237Z",
     "start_time": "2019-11-27T12:48:08.788737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "max_len = max(input_length)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:48:13.816815Z",
     "start_time": "2019-11-27T12:48:13.811185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 64, 512])\n",
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.shape)\n",
    "print(problem_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T11:02:53.829504Z",
     "start_time": "2019-11-27T11:02:53.824753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 53, 53, 50, 48, 48, 45, 44, 43, 42, 42, 41, 39, 39, 38, 38, 38, 34, 34, 34, 34, 34, 33, 33, 33, 32, 32, 32, 31, 30, 30, 28, 28, 28, 27, 27, 26, 26, 25, 25, 25, 24, 24, 23, 23, 23, 23, 23, 23, 22, 22, 22, 21, 21, 21, 19, 18, 17, 16, 16, 15, 13, 11, 6]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:40:41.424118Z",
     "start_time": "2019-11-27T08:40:41.418865Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 11, 12, 13, 7, 14, 15, 1, 16, 17, 18, 19, 20, 21, 22, 17, 23, 24, 25, 26, 27, 28, 29, 17, 23, 1, 30, 26, 31, 32, 10, 33, 34, 16, 13], 44, [0, 1, 8, 5, 7], 5, ['2', '11'], [16, 34], [], [15, 16, 17, 32, 33, 34, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:35:02.326254Z",
     "start_time": "2019-11-27T08:35:02.319874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['*', '-', 'N1', '1', 'N0'], ['2', '11'], [16, 34], [15, 16, 17, 32, 33, 34, 39, 40, 41])\n",
      "(['张', '明', '有', 'NUM', '元', '钱', '，', '买', '书', '用', '去', 'NUM', '，', '买', '文具', '的', '钱', '是', '买', '书', '的', 'NUM', '．', '买', '文具', '用', '去', '多少', '元', '？'], ['*', '*', 'N0', 'N1', 'N2'], ['120', '80%', '15%'], [3, 11, 21], [1, 2, 3, 4, 7, 8, 9, 18, 19, 20, 21, 27, 28])\n"
     ]
    }
   ],
   "source": [
    "print(pairs_trained[0])\n",
    "print(pairs_tested[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:30:14.893229Z",
     "start_time": "2019-11-27T08:30:14.877875Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,j in zip(data,group_data):\n",
    "    if i['id'] != j['id']:\n",
    "        print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T07:41:06.300228Z",
     "start_time": "2019-11-27T07:41:06.293797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'original_text': '镇海雅乐学校二年级的小朋友到一条小路的一边植树．小朋友们每隔2米种一棵树（马路两头都种了树），最后发现一共种了11棵，这条小路长多少米．', 'segmented_text': '镇海 雅乐 学校 二年级 的 小朋友 到 一条 小路 的 一边 植树 ． 小朋友 们 每隔 2 米 种 一棵树 （ 马路 两头 都 种 了 树 ） ， 最后 发现 一共 种 了 11 棵 ， 这 条 小路 长 多少 米 ．', 'equation': 'x=(11-1)*2', 'ans': '20'}\n",
      "{'id': '1', 'group_num': [15, 16, 17, 32, 33, 34, 39, 40, 41]}\n",
      "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['*', '-', 'N1', '1', 'N0'], ['2', '11'], [16, 34])\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(group_data[0])\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T07:54:35.750909Z",
     "start_time": "2019-11-27T07:54:35.744948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM\n",
      "NUM\n"
     ]
    }
   ],
   "source": [
    "print(pairs_trained[0][0][16])\n",
    "print(pairs_trained[0][0][34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T07:46:21.695051Z",
     "start_time": "2019-11-27T07:46:11.351970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing words...\n",
      "keep_words 3928 / 10543 = 0.3726\n",
      "Indexed 3931 words in input language, 23 words in output\n",
      "Number of training data 21162\n",
      "Number of testind data 1000\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, train_pairs, test_pairs = prepare_data(pairs_trained, pairs_tested, 5, generate_nums,\n",
    "                                                                copy_nums, tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:02:13.129194Z",
     "start_time": "2019-11-27T08:02:13.123098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([35, 36, 37, 38, 26, 39, 40, 23, 1, 41, 26, 42, 43, 44, 45, 46, 47, 22, 40, 1, 41, 26, 48, 40, 23, 1, 45, 26, 49, 37, 50, 51, 38, 34, 41, 52], 36, [2, 7, 0, 8, 1, 9, 5], 7, ['316', '230', '6'], [8, 19, 25], [])\n",
      "------------------------------------------------------------\n",
      "([207, 35, 796, 2, 6, 1, 197, 481, 23, 1, 30, 484, 26, 58, 3269, 484, 1088, 6, 1903, 71, 1, 16, 26, 49, 796, 6, 439, 75, 34, 16, 52], 31, [0, 8, 9], 3, ['4', '44', '20'], [5, 9, 20], [])\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[1])\n",
    "print('-'*60)\n",
    "print(test_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T07:47:07.673781Z",
     "start_time": "2019-11-27T07:47:07.668228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pairs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=embedding_size, hidden_size=hidden_size,\n",
    "                     n_layers=n_layers)\n",
    "predict = Prediction(hidden_size=hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums),\n",
    "                     input_size=len(generate_nums))\n",
    "generate = GenerateNode(hidden_size=hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums),\n",
    "                        embedding_size=embedding_size)\n",
    "merge = Merge(hidden_size=hidden_size, embedding_size=embedding_size)\n",
    "# the embedding layer is  only for generated number embeddings, operators, and paddings\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "predict_optimizer = torch.optim.Adam(predict.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "generate_optimizer = torch.optim.Adam(generate.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "merge_optimizer = torch.optim.Adam(merge.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n",
    "predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n",
    "generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n",
    "merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    predict.cuda()\n",
    "    generate.cuda()\n",
    "    merge.cuda()\n",
    "\n",
    "generate_num_ids = []\n",
    "for num in generate_nums:\n",
    "    generate_num_ids.append(output_lang.word2index[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(n_epochs):\n",
    "encoder_scheduler.step()\n",
    "predict_scheduler.step()\n",
    "generate_scheduler.step()\n",
    "merge_scheduler.step()\n",
    "loss_total = 0\n",
    "input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, \\\n",
    "    num_pos_batches, num_size_batches = prepare_train_batch(train_pairs, batch_size)\n",
    "#print(\"fold:\", fold + 1)\n",
    "#print(\"epoch:\", epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "[50, 49, 47, 44, 41, 38, 36, 36, 36, 35, 35, 34, 34, 34, 33, 33, 33, 32, 32, 31, 31, 30, 29, 29, 29, 28, 28, 27, 27, 27, 27, 27, 27, 26, 26, 25, 25, 24, 24, 24, 24, 22, 22, 22, 21, 21, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 17, 17, 17, 15, 14, 11, 11, 10]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(input_lengths))\n",
    "print(input_lengths[0])\n",
    "print(max(input_lengths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 5, 5, 4, 4, 2, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3, 5, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 1, 2, 2, 1, 2, 3, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(num_size_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,\n",
    "               encoder, predict, generate, merge, encoder_optimizer, predict_optimizer, generate_optimizer,\n",
    "               merge_optimizer, output_lang, num_pos) = (input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n",
    "    num_stack_batches[idx], num_size_batches[idx], generate_num_ids, encoder, predict, generate, merge,\n",
    "    encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n",
    "english=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence mask for attention\n",
    "seq_mask = []\n",
    "max_len = max(input_length)\n",
    "for i in input_length:\n",
    "    seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n",
    "seq_mask = torch.ByteTensor(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mask = []\n",
    "max_num_size = max(num_size_batch) + len(generate_nums)\n",
    "for i in num_size_batch:\n",
    "    d = i + len(generate_nums)\n",
    "    num_mask.append([0] * d + [1] * (max_num_size - d))\n",
    "num_mask = torch.ByteTensor(num_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = output_lang.word2index[\"UNK\"]\n",
    "\n",
    "# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "input_var = torch.LongTensor(input_batch).transpose(0, 1)\n",
    "\n",
    "target = torch.LongTensor(target_batch).transpose(0, 1)\n",
    "\n",
    "padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n",
    "batch_size = len(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(padding_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50])\n",
      "torch.Size([50, 64])\n",
      "torch.Size([64, 9])\n",
      "torch.Size([9, 64])\n"
     ]
    }
   ],
   "source": [
    "print(torch.LongTensor(input_batch).shape)\n",
    "print(input_var.shape)\n",
    "print(torch.LongTensor(target_batch).shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderSeq(\n",
      "  (embedding): Embedding(3674, 128, padding_idx=0)\n",
      "  (em_dropout): Dropout(p=0.5)\n",
      "  (gru_pade): GRU(128, 512, num_layers=2, dropout=0.5, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "predict.train()\n",
    "generate.train()\n",
    "merge.train()\n",
    "\n",
    "if USE_CUDA:\n",
    "    input_var = input_var.cuda()\n",
    "    seq_mask = seq_mask.cuda()\n",
    "    padding_hidden = padding_hidden.cuda()\n",
    "    num_mask = num_mask.cuda()\n",
    "\n",
    "# Zero gradients of both optimizers\n",
    "encoder_optimizer.zero_grad()\n",
    "predict_optimizer.zero_grad()\n",
    "generate_optimizer.zero_grad()\n",
    "merge_optimizer.zero_grad()\n",
    "# Run words through encoder\n",
    "\n",
    "encoder_outputs, problem_output = encoder(input_var, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 64, 512])\n",
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.shape)\n",
    "print(problem_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for idx in range(len(input_lengths)):\n",
    "    loss = train_tree(\n",
    "        input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n",
    "        num_stack_batches[idx], num_size_batches[idx], generate_num_ids, encoder, predict, generate, merge,\n",
    "        encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx])\n",
    "    loss_total += loss\n",
    "\n",
    "print(\"loss:\", loss_total / len(input_lengths))\n",
    "print(\"training time\", time_since(time.time() - start))\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['时代', '超市', '“', 'NUM', '一', '”', '大', '酬宾', '，', '全场', '家电', '按', 'NUM', '销售', '，', '原价', 'NUM', '元', '的', '电饭锅', '，', '现在', '售价', '=', '多少', '元', '．'], ['*', 'N2', 'N1'], ['5', '0.8', '150'], [3, 12, 16])\n"
     ]
    }
   ],
   "source": [
    "print(pairs_trained[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'original_text': '镇海雅乐学校二年级的小朋友到一条小路的一边植树．小朋友们每隔2米种一棵树（马路两头都种了树），最后发现一共种了11棵，这条小路长多少米．', 'segmented_text': '镇海 雅乐 学校 二年级 的 小朋友 到 一条 小路 的 一边 植树 ． 小朋友 们 每隔 2 米 种 一棵树 （ 马路 两头 都 种 了 树 ） ， 最后 发现 一共 种 了 11 棵 ， 这 条 小路 长 多少 米 ．', 'equation': 'x=(11-1)*2', 'ans': '20'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = '../graph_quantity_multigraph_trans/data/'\n",
    "prefix = '23k_processed.json'\n",
    "def get_train_test_fold(ori_path,prefix,data,pairs):\n",
    "    mode_train = 'train'\n",
    "    mode_valid = 'valid'\n",
    "    mode_test = 'test'\n",
    "    train_path = ori_path + mode_train + prefix\n",
    "    valid_path = ori_path + mode_valid + prefix\n",
    "    test_path = ori_path + mode_test + prefix\n",
    "    train = read_json(train_path)\n",
    "    train_id = [item['id'] for item in train]\n",
    "    valid = read_json(valid_path)\n",
    "    valid_id = [item['id'] for item in valid]\n",
    "    test = read_json(test_path)\n",
    "    test_id = [item['id'] for item in test]\n",
    "    train_fold = []\n",
    "    valid_fold = []\n",
    "    test_fold = []\n",
    "    for item,pair in zip(data, pairs):\n",
    "        if item['id'] in train_id:\n",
    "            train_fold.append(pair)\n",
    "        elif item['id'] in test_id:\n",
    "            test_fold.append(pair)\n",
    "        else:\n",
    "            valid_fold.append(pair)\n",
    "    return train_fold, test_fold, valid_fold\n",
    "train_fold, test_fold, valid_fold = get_train_test_fold(ori_path,prefix,data, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['镇海', '雅乐', '学校', '二年级', '的', '小朋友', '到', '一条', '小路', '的', '一边', '植树', '．', '小朋友', '们', '每隔', 'NUM', '米', '种', '一棵树', '（', '马路', '两头', '都', '种', '了', '树', '）', '，', '最后', '发现', '一共', '种', '了', 'NUM', '棵', '，', '这', '条', '小路', '长', '多少', '米', '．'], ['*', '-', 'N1', '1', 'N0'], ['2', '11'], [16, 34])\n"
     ]
    }
   ],
   "source": [
    "print(train_fold[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
