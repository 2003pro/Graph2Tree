{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:12:59.616797Z",
     "start_time": "2019-11-27T12:12:58.270238Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from src.train_and_evaluate import *\n",
    "from src.models import *\n",
    "import time\n",
    "import torch.optim\n",
    "from src.expressions_transfer import *\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path,'r') as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "def write_json(path,file):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:13:18.841736Z",
     "start_time": "2019-11-27T12:13:18.829288Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_fold(ori_path,prefix,data,pairs,group):\n",
    "    mode_train = 'train'\n",
    "    mode_valid = 'valid'\n",
    "    mode_test = 'test'\n",
    "    train_path = ori_path + mode_train + prefix\n",
    "    valid_path = ori_path + mode_valid + prefix\n",
    "    test_path = ori_path + mode_test + prefix\n",
    "    train = read_json(train_path)\n",
    "    train_id = [item['id'] for item in train]\n",
    "    valid = read_json(valid_path)\n",
    "    valid_id = [item['id'] for item in valid]\n",
    "    test = read_json(test_path)\n",
    "    test_id = [item['id'] for item in test]\n",
    "    train_fold = []\n",
    "    valid_fold = []\n",
    "    test_fold = []\n",
    "    for item,pair,g in zip(data, pairs, group):\n",
    "        pair = list(pair)\n",
    "        pair.append(g['group_num'])\n",
    "        pair = tuple(pair)\n",
    "        if item['id'] in train_id:\n",
    "            train_fold.append(pair)\n",
    "        elif item['id'] in test_id:\n",
    "            test_fold.append(pair)\n",
    "        else:\n",
    "            valid_fold.append(pair)\n",
    "    return train_fold, test_fold, valid_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:13:19.789441Z",
     "start_time": "2019-11-27T12:13:19.780071Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_raw_data(filename):  # load the json data to list(dict()) for MATH 23K\n",
    "    print(\"Reading lines...\")\n",
    "    f = open(filename, encoding=\"utf-8\")\n",
    "    js = \"\"\n",
    "    data = []\n",
    "    for i, s in enumerate(f):\n",
    "        js += s\n",
    "        i += 1\n",
    "        if i % 7 == 0:  # every 7 line is a json\n",
    "            data_d = json.loads(js)\n",
    "            if \"千米/小时\" in data_d[\"equation\"]:\n",
    "                data_d[\"equation\"] = data_d[\"equation\"][:-5]\n",
    "            data.append(data_d)\n",
    "            js = \"\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:13:42.590706Z",
     "start_time": "2019-11-27T12:13:42.567440Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums,group_train, group_test, tree=False):\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs_trained:\n",
    "        if not tree:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "        elif pair[-1]:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "    input_lang.build_input_lang(trim_min_count)\n",
    "    if tree:\n",
    "        output_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n",
    "    else:\n",
    "        output_lang.build_output_lang(generate_nums, copy_nums)\n",
    "\n",
    "    for pair in pairs_trained:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                            pair[2], pair[3], num_stack))\n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    print('Number of training data %d' % (len(train_pairs)))\n",
    "    for pair in pairs_tested:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        test_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                           pair[2], pair[3], num_stack))\n",
    "    print('Number of testind data %d' % (len(test_pairs)))\n",
    "    return input_lang, output_lang, train_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:45:34.461173Z",
     "start_time": "2019-11-27T12:45:34.449355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5, 0.02, 2.0]\n"
     ]
    }
   ],
   "source": [
    "def change_num(num):\n",
    "    new_num = []\n",
    "    for item in num:\n",
    "        if '/' in item:\n",
    "            new_str = item.split(')')[0]\n",
    "            new_str = new_str.split('(')[1]\n",
    "            a = float(new_str.split('/')[0])\n",
    "            b = float(new_str.split('/')[1])\n",
    "            value = a/b\n",
    "            new_num.append(value)\n",
    "        elif '%' in item:\n",
    "            value = float(item[0:-1])/100\n",
    "            new_num.append(value)\n",
    "        else:\n",
    "            new_num.append(float(item))\n",
    "    return new_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:04.432750Z",
     "start_time": "2019-11-27T12:45:54.097513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n",
      "Transfer numbers...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 128\n",
    "hidden_size = 512\n",
    "n_epochs = 80\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "beam_size = 5\n",
    "n_layers = 2\n",
    "ori_path = '../graph_quantity_multigraph_trans/data/'\n",
    "prefix = '23k_processed.json'\n",
    "\n",
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")\n",
    "group_data = read_json(\"data/Math_23K_processed.json\")\n",
    "\n",
    "data = load_raw_data(\"data/Math_23K.json\")\n",
    "\n",
    "pairs, generate_nums, copy_nums = transfer_num(data)\n",
    "\n",
    "temp_pairs = []\n",
    "for p in pairs:\n",
    "    temp_pairs.append((p[0], from_infix_to_prefix(p[1]), change_num(p[2]), p[3]))\n",
    "pairs = temp_pairs\n",
    "\n",
    "train_fold, test_fold, valid_fold = get_train_test_fold(ori_path,prefix,data,pairs,group_data)\n",
    "\n",
    "\n",
    "best_acc_fold = []\n",
    "\n",
    "pairs_tested = test_fold\n",
    "pairs_trained = train_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:49.864021Z",
     "start_time": "2019-11-27T12:46:39.306073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing words...\n",
      "keep_words 3928 / 10543 = 0.3726\n",
      "Indexed 3931 words in input language, 23 words in output\n",
      "Number of training data 21162\n",
      "Number of testind data 1000\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(pairs_trained, pairs_tested, trim_min_count, generate_nums, copy_nums, tree=False):\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    train_pairs = []\n",
    "    test_pairs = []\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs_trained:\n",
    "        if not tree:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "        elif pair[-1]:\n",
    "            input_lang.add_sen_to_vocab(pair[0])\n",
    "            output_lang.add_sen_to_vocab(pair[1])\n",
    "    input_lang.build_input_lang(trim_min_count)\n",
    "    if tree:\n",
    "        output_lang.build_output_lang_for_tree(generate_nums, copy_nums)\n",
    "    else:\n",
    "        output_lang.build_output_lang(generate_nums, copy_nums)\n",
    "\n",
    "    for pair in pairs_trained:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                            pair[2], pair[3], num_stack, pair[4]))\n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    print('Number of training data %d' % (len(train_pairs)))\n",
    "    for pair in pairs_tested:\n",
    "        num_stack = []\n",
    "        for word in pair[1]:\n",
    "            temp_num = []\n",
    "            flag_not = True\n",
    "            if word not in output_lang.index2word:\n",
    "                flag_not = False\n",
    "                for i, j in enumerate(pair[2]):\n",
    "                    if j == word:\n",
    "                        temp_num.append(i)\n",
    "\n",
    "            if not flag_not and len(temp_num) != 0:\n",
    "                num_stack.append(temp_num)\n",
    "            if not flag_not and len(temp_num) == 0:\n",
    "                num_stack.append([_ for _ in range(len(pair[2]))])\n",
    "\n",
    "        num_stack.reverse()\n",
    "        input_cell = indexes_from_sentence(input_lang, pair[0])\n",
    "        output_cell = indexes_from_sentence(output_lang, pair[1], tree)\n",
    "        # train_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "        #                     pair[2], pair[3], num_stack, pair[4]))\n",
    "        test_pairs.append((input_cell, len(input_cell), output_cell, len(output_cell),\n",
    "                           pair[2], pair[3], num_stack,pair[4]))\n",
    "    print('Number of testind data %d' % (len(test_pairs)))\n",
    "    return input_lang, output_lang, train_pairs, test_pairs\n",
    "input_lang, output_lang, train_pairs, test_pairs = prepare_data(pairs_trained, pairs_tested, 5, generate_nums,\n",
    "                                                                copy_nums, tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:14:08.886494Z",
     "start_time": "2019-11-27T12:14:08.876602Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_train_batch(pairs_to_batch, batch_size):\n",
    "    pairs = copy.deepcopy(pairs_to_batch)\n",
    "    random.shuffle(pairs)  # shuffle the pairs\n",
    "    pos = 0\n",
    "    input_lengths = []\n",
    "    output_lengths = []\n",
    "    nums_batches = []\n",
    "    batches = []\n",
    "    input_batches = []\n",
    "    output_batches = []\n",
    "    num_stack_batches = []  # save the num stack which\n",
    "    num_pos_batches = []\n",
    "    num_size_batches = []\n",
    "    group_batches = []\n",
    "    num_value_batches = []\n",
    "    while pos + batch_size < len(pairs):\n",
    "        batches.append(pairs[pos:pos+batch_size])\n",
    "        pos += batch_size\n",
    "    batches.append(pairs[pos:])\n",
    "\n",
    "    for batch in batches:\n",
    "        batch = sorted(batch, key=lambda tp: tp[1], reverse=True)\n",
    "        input_length = []\n",
    "        output_length = []\n",
    "        for _, i, _, j, _, _, _,_ in batch:\n",
    "            input_length.append(i)\n",
    "            output_length.append(j)\n",
    "        input_lengths.append(input_length)\n",
    "        output_lengths.append(output_length)\n",
    "        input_len_max = input_length[0]\n",
    "        output_len_max = max(output_length)\n",
    "        input_batch = []\n",
    "        output_batch = []\n",
    "        num_batch = []\n",
    "        num_stack_batch = []\n",
    "        num_pos_batch = []\n",
    "        num_size_batch = []\n",
    "        group_batch = []\n",
    "        num_value_batch = []\n",
    "        for i, li, j, lj, num, num_pos, num_stack, group in batch:\n",
    "            num_batch.append(len(num))\n",
    "            input_batch.append(pad_seq(i, li, input_len_max))\n",
    "            output_batch.append(pad_seq(j, lj, output_len_max))\n",
    "            num_stack_batch.append(num_stack)\n",
    "            num_pos_batch.append(num_pos)\n",
    "            num_size_batch.append(len(num_pos))\n",
    "            num_value_batch.append(num)\n",
    "            group_batch.append(group)\n",
    "        input_batches.append(input_batch)\n",
    "        nums_batches.append(num_batch)\n",
    "        output_batches.append(output_batch)\n",
    "        num_stack_batches.append(num_stack_batch)\n",
    "        num_pos_batches.append(num_pos_batch)\n",
    "        num_size_batches.append(num_size_batch)\n",
    "        num_value_batches.append(num_value_batch)\n",
    "        group_batches.append(group_batch)\n",
    "        \n",
    "    return input_batches, input_lengths, output_batches, output_lengths, nums_batches, num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, group_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:51.135145Z",
     "start_time": "2019-11-27T12:46:49.866107Z"
    }
   },
   "outputs": [],
   "source": [
    "input_batches, input_lengths, output_batches, output_lengths, nums_batches, \\\n",
    "   num_stack_batches, num_pos_batches, num_size_batches, num_value_batches, group_batches = prepare_train_batch(train_pairs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Test Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T10:59:11.915005Z",
     "start_time": "2019-11-27T10:59:11.911506Z"
    }
   },
   "source": [
    "## Build Model and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:53.316259Z",
     "start_time": "2019-11-27T12:46:53.113759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = EncoderSeq(input_size=input_lang.n_words, embedding_size=embedding_size, hidden_size=hidden_size,\n",
    "                     n_layers=n_layers)\n",
    "predict = Prediction(hidden_size=hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums),\n",
    "                     input_size=len(generate_nums))\n",
    "generate = GenerateNode(hidden_size=hidden_size, op_nums=output_lang.n_words - copy_nums - 1 - len(generate_nums),\n",
    "                        embedding_size=embedding_size)\n",
    "merge = Merge(hidden_size=hidden_size, embedding_size=embedding_size)\n",
    "# the embedding layer is  only for generated number embeddings, operators, and paddings\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "predict_optimizer = torch.optim.Adam(predict.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "generate_optimizer = torch.optim.Adam(generate.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "merge_optimizer = torch.optim.Adam(merge.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=20, gamma=0.5)\n",
    "predict_scheduler = torch.optim.lr_scheduler.StepLR(predict_optimizer, step_size=20, gamma=0.5)\n",
    "generate_scheduler = torch.optim.lr_scheduler.StepLR(generate_optimizer, step_size=20, gamma=0.5)\n",
    "merge_scheduler = torch.optim.lr_scheduler.StepLR(merge_optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    predict.cuda()\n",
    "    generate.cuda()\n",
    "    merge.cuda()\n",
    "\n",
    "generate_num_ids = []\n",
    "for num in generate_nums:\n",
    "    generate_num_ids.append(output_lang.word2index[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:14:19.404905Z",
     "start_time": "2019-11-27T12:14:19.401911Z"
    }
   },
   "outputs": [],
   "source": [
    "#for epoch in range(n_epochs):\n",
    "encoder_scheduler.step()\n",
    "predict_scheduler.step()\n",
    "generate_scheduler.step()\n",
    "merge_scheduler.step()\n",
    "loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:54.417978Z",
     "start_time": "2019-11-27T12:46:54.408982Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "(input_batch, input_length, target_batch, target_length, nums_stack_batch, num_size_batch, generate_nums,\n",
    "               encoder, predict, generate, merge, encoder_optimizer, predict_optimizer, generate_optimizer,\n",
    "               merge_optimizer, output_lang, num_pos, num_value, group) = (input_batches[idx], input_lengths[idx], output_batches[idx], output_lengths[idx],\n",
    "    num_stack_batches[idx], num_size_batches[idx], generate_num_ids, encoder, predict, generate, merge,\n",
    "    encoder_optimizer, predict_optimizer, generate_optimizer, merge_optimizer, output_lang, num_pos_batches[idx], num_value_batches[idx], group_batches[idx])\n",
    "english=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:46:58.125460Z",
     "start_time": "2019-11-27T12:46:58.120420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 50, 47, 46, 45, 44, 42, 42, 41, 39, 36, 33, 32, 32, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 27, 27, 27, 26, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 21, 20, 20, 18, 17, 17, 17, 17, 15, 15, 11, 11, 10, 10, 9]\n"
     ]
    }
   ],
   "source": [
    "print(input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_tree Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:00.262425Z",
     "start_time": "2019-11-27T12:47:00.255401Z"
    }
   },
   "outputs": [],
   "source": [
    "# sequence mask for attention\n",
    "seq_mask = []\n",
    "max_len = max(input_length)\n",
    "for i in input_length:\n",
    "    seq_mask.append([0 for _ in range(i)] + [1 for _ in range(i, max_len)])\n",
    "seq_mask = torch.ByteTensor(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:02.748740Z",
     "start_time": "2019-11-27T12:47:02.738472Z"
    }
   },
   "outputs": [],
   "source": [
    "num_mask = []\n",
    "max_num_size = max(num_size_batch) + len(generate_nums)\n",
    "for i in num_size_batch:\n",
    "    d = i + len(generate_nums)\n",
    "    num_mask.append([0] * d + [1] * (max_num_size - d))\n",
    "num_mask = torch.ByteTensor(num_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:03.760241Z",
     "start_time": "2019-11-27T12:47:03.752171Z"
    }
   },
   "outputs": [],
   "source": [
    "unk = output_lang.word2index[\"UNK\"]\n",
    "\n",
    "# Turn padded arrays into (batch_size x max_len) tensors, transpose into (max_len x batch_size)\n",
    "input_var = torch.LongTensor(input_batch).transpose(0, 1)\n",
    "\n",
    "target = torch.LongTensor(target_batch).transpose(0, 1)\n",
    "\n",
    "padding_hidden = torch.FloatTensor([0.0 for _ in range(predict.hidden_size)]).unsqueeze(0)\n",
    "batch_size = len(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:04.998165Z",
     "start_time": "2019-11-27T12:47:04.975830Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "predict.train()\n",
    "generate.train()\n",
    "merge.train()\n",
    "\n",
    "if USE_CUDA:\n",
    "    input_var = input_var.cuda()\n",
    "    seq_mask = seq_mask.cuda()\n",
    "    padding_hidden = padding_hidden.cuda()\n",
    "    num_mask = num_mask.cuda()\n",
    "\n",
    "# Zero gradients of both optimizers\n",
    "encoder_optimizer.zero_grad()\n",
    "predict_optimizer.zero_grad()\n",
    "generate_optimizer.zero_grad()\n",
    "merge_optimizer.zero_grad()\n",
    "# Run words through encoder\n",
    "\n",
    "encoder_outputs, problem_output = encoder(input_var, input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:08:45.496833Z",
     "start_time": "2019-11-27T13:08:45.476718Z"
    }
   },
   "outputs": [],
   "source": [
    "# num net graph\n",
    "def get_lower_num_graph(max_len, sentence_length, num_list, id_num_list,contain_zh_flag=True):\n",
    "    diag_ele = np.zeros(max_len)\n",
    "    for i in range(sentence_length):\n",
    "        diag_ele[i] = 1\n",
    "    graph = np.diag(diag_ele)\n",
    "    if not contain_zh_flag:\n",
    "        return graph\n",
    "    for i in range(len(id_num_list)):\n",
    "        for j in range(len(id_num_list)):\n",
    "            if float(num_list[i]) <= float(num_list[j]):\n",
    "                graph[id_num_list[i]][id_num_list[j]] = 1\n",
    "            else:\n",
    "                graph[id_num_list[j]][id_num_list[i]] = 1\n",
    "    return graph\n",
    "\n",
    "def get_greater_num_graph(max_len, sentence_length, num_list, id_num_list,contain_zh_flag=True):\n",
    "    diag_ele = np.zeros(max_len)\n",
    "    for i in range(sentence_length):\n",
    "        diag_ele[i] = 1\n",
    "    graph = np.diag(diag_ele)\n",
    "    if not contain_zh_flag:\n",
    "        return graph\n",
    "    for i in range(len(id_num_list)):\n",
    "        for j in range(len(id_num_list)):\n",
    "            if float(num_list[i]) > float(num_list[j]):\n",
    "                graph[id_num_list[i]][id_num_list[j]] = 1\n",
    "            else:\n",
    "                graph[id_num_list[j]][id_num_list[i]] = 1\n",
    "    return graph\n",
    "\n",
    "# quantity cell graph\n",
    "def get_quantity_graph(max_len, sentence_length, quantity_cell_list,contain_zh_flag=True):\n",
    "    diag_ele = np.zeros(max_len)\n",
    "    for i in range(sentence_length):\n",
    "        diag_ele[i] = 1\n",
    "    graph = np.diag(diag_ele)\n",
    "    if not contain_zh_flag:\n",
    "        return graph\n",
    "    for i in quantity_cell_list:\n",
    "        for j in quantity_cell_list:\n",
    "            graph[i][j] = 1\n",
    "            graph[j][i] = 1\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:47:29.230668Z",
     "start_time": "2019-11-27T12:47:29.143894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_batch_graph(input_length,group,num_value,num_pos):\n",
    "    batch_graph = []\n",
    "    max_len = max(input_length)\n",
    "    for i in range(len(input_length)):\n",
    "        sentence_length = input_length[i]\n",
    "        quantity_cell_list = group[i]\n",
    "        num_list = num_value[i]\n",
    "        id_num_list = num_pos[i]\n",
    "        graph_newc = get_quantity_graph(max_len, sentence_length, quantity_cell_list)\n",
    "        graph_greater = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n",
    "        graph_lower = get_greater_num_graph(max_len, sentence_length, num_list, id_num_list)\n",
    "        graph_total = [graph_newc.tolist(),graph_greater.tolist(),graph_lower.tolist()]\n",
    "        batch_graph.append(graph_total)\n",
    "    batch_graph = np.array(batch_graph)\n",
    "    return batch_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:50:01.732219Z",
     "start_time": "2019-11-27T12:50:01.673814Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gcn = Graph_Module(hidden_size, hidden_size, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:53:53.688463Z",
     "start_time": "2019-11-27T12:53:53.683082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:54:10.599260Z",
     "start_time": "2019-11-27T12:54:10.594242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 51, 51])\n"
     ]
    }
   ],
   "source": [
    "print(batch_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:54:39.328277Z",
     "start_time": "2019-11-27T12:54:39.322973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 51, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.transpose(0, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:04:12.414641Z",
     "start_time": "2019-11-27T13:04:12.353736Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gcn = Graph_Module(hidden_size, hidden_size, hidden_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:05:16.189549Z",
     "start_time": "2019-11-27T13:05:16.153309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 512])\n",
      "adj.shape\n",
      "torch.Size([64, 51, 51])\n",
      "support.shape\n",
      "torch.Size([64, 51, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs = encoder_outputs.transpose(0, 1).cuda()\n",
    "batch_graph = batch_graph.cuda()\n",
    "_, encoder_outputs = gcn(encoder_outputs, batch_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:59:23.823874Z",
     "start_time": "2019-11-27T12:59:23.818559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 51, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:48:08.794237Z",
     "start_time": "2019-11-27T12:48:08.788737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "max_len = max(input_length)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:48:13.816815Z",
     "start_time": "2019-11-27T12:48:13.811185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 64, 512])\n",
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.shape)\n",
    "print(problem_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T11:02:53.829504Z",
     "start_time": "2019-11-27T11:02:53.824753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 53, 53, 50, 48, 48, 45, 44, 43, 42, 42, 41, 39, 39, 38, 38, 38, 34, 34, 34, 34, 34, 33, 33, 33, 32, 32, 32, 31, 30, 30, 28, 28, 28, 27, 27, 26, 26, 25, 25, 25, 24, 24, 23, 23, 23, 23, 23, 23, 22, 22, 22, 21, 21, 21, 19, 18, 17, 16, 16, 15, 13, 11, 6]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
